{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9I9rz0pTamX"
   },
   "source": [
    "# XNLI Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiowR0WNTd1C"
   },
   "source": [
    "In this notebook, we are going to train a model for evaluating on [XNLI](https://github.com/facebookresearch/XNLI). XNLI a cross-lingual NLI task, spanning 15 different languages, with 10,000 validation and test examples per language. Notably, XNLI does not have its own training set - instead, the usual recipe is to MNLI as a training set, and is then zero-shot evaluated on NLI examples in other languages. Of course, this works best when you start with a model that has already been pretrained on a lot of multi-lingual text, such as mBERT or XLM/XLM-RoBERTa.\n",
    "\n",
    "Hence, the tricky part about this setup is that although we have separate XNLI and MNLI tasks, we need them to all use the same task head. We will cover how to easily do this with `jiant`.\n",
    "\n",
    "--- \n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "* Train an XLM-RoBERTa base model on MNLI\n",
    "* Evaluate on XNLI-de (German) and XNLI-zh (Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXbD_U1_VDnw"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tC9teoazUnW8"
   },
   "source": [
    "#### Install dependencies\n",
    "\n",
    "First, we will install libraries we need for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aU3Z9szuMU9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/nyu-mll/jiant.git\n",
    "%cd jiant\n",
    "!pip install -r requirements-no-torch.txt\n",
    "!pip install --no-deps -e ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGJcCmRzU1Qb"
   },
   "source": [
    "#### Download data\n",
    "\n",
    "Next, we will download MNLI and XNLI data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKCz8VksvFlN"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd /content\n",
    "# Download MNLI and XNLI data\n",
    "!PYTHONPATH=/content/jiant python jiant/jiant/scripts/download_data/runscript.py \\\n",
    "    download \\\n",
    "    --tasks mnli xnli \\\n",
    "    --output_path=/content/tasks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQKSAhYzVIlv"
   },
   "source": [
    "## `jiant` Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v88oXqmBvFuK"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../../jiant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "!CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibmMT7CXv1_P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import jiant.proj.main.tokenize_and_cache as tokenize_and_cache\n",
    "import jiant.proj.main.export_model as export_model\n",
    "import jiant.proj.main.scripts.configurator as configurator\n",
    "import jiant.proj.main.runscript as main_runscript\n",
    "import jiant.shared.caching as caching\n",
    "import jiant.utils.python.io as py_io\n",
    "import jiant.utils.display as display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPZHyLOlVp07"
   },
   "source": [
    "#### Download model\n",
    "\n",
    "Next, we will download an `xlm-roberta-base` model. This also includes the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K06qUGjkKWa7"
   },
   "outputs": [],
   "source": [
    "export_model.export_model(\n",
    "    hf_pretrained_model_name_or_path=\"xlm-roberta-base\",\n",
    "    output_base_path=\"./models/xlm-roberta-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dV-T-8r1V0wf"
   },
   "source": [
    "#### Tokenize and cache\n",
    "\n",
    "With the model and data ready, we can now tokenize and cache the inputs features for our tasks. This converts the input examples to tokenized features ready to be consumed by the model, and saved them to disk in chunks.\n",
    "\n",
    "Note that we are tokenize `train` and `val` data for MNLI, but only `val` data for XNLI, since there is no corresponding training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs  data  disrpt_cfg.tgz  ICSI_split_da_09_08_22.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!ls ../../exp/tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22bNWQajO4zm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnliTask\n",
      "  [train]: /moredata/muller/Devel/jiant/exp/tasks/data/mnli/train.jsonl\n",
      "  [val]: /moredata/muller/Devel/jiant/exp/tasks/data/mnli/val.jsonl\n",
      "  [test]: /moredata/muller/Devel/jiant/exp/tasks/data/mnli/test.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7528cc60a9564d659d4c656bf777456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/392702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70ce67d31a64582bba0d701e841bbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/9815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XnliTask\n",
      "  [val]: /moredata/muller/Devel/jiant/exp/tasks/data/xnli_de/val.jsonl\n",
      "  [test]: /moredata/muller/Devel/jiant/exp/tasks/data/xnli_de/test.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ced19c9bc4d46ca98aa5cf6caf672de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XnliTask\n",
      "  [val]: /moredata/muller/Devel/jiant/exp/tasks/data/xnli_zh/val.jsonl\n",
      "  [test]: /moredata/muller/Devel/jiant/exp/tasks/data/xnli_zh/test.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1af728f92c48a3a6639e5fdb75f567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize and cache MNLI\n",
    "tokenize_and_cache.main(tokenize_and_cache.RunConfiguration(\n",
    "    task_config_path=f\"../../exp/tasks/configs/mnli_config.json\",\n",
    "    hf_pretrained_model_name_or_path=\"xlm-roberta-base\",\n",
    "    output_dir=f\"../../exp/cache/mnli\",\n",
    "    phases=[\"train\", \"val\"],\n",
    "))\n",
    "\n",
    "# Tokenize and cache XNLI-de, XNLI-zh\n",
    "for lang in [\"de\", \"zh\"]:\n",
    "    tokenize_and_cache.main(tokenize_and_cache.RunConfiguration(\n",
    "        task_config_path=f\"../../exp/tasks/configs/xnli_{lang}_config.json\",\n",
    "        hf_pretrained_model_name_or_path=\"xlm-roberta-base\",\n",
    "        output_dir=f\"../../exp/cache/xnli_{lang}\",\n",
    "        phases=[\"val\"],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ-mWSQQWJsw"
   },
   "source": [
    "We can inspect the first examples of the first chunk of each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLk_X0KypUyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0 128182     34  25958  24709   7158  58838   1556   6626  62822\n",
      " 158208     20  12996    136    700  87168      5      2      2  73111\n",
      "    136    700  87168    621   2367   3249  24709   7158  58838   4488\n",
      "      5      2      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1]\n",
      "['<s>', '▁Concept', 'u', 'ally', '▁cream', '▁ski', 'mming', '▁has', '▁two', '▁basic', '▁dimensions', '▁-', '▁product', '▁and', '▁ge', 'ography', '.', '</s>', '</s>', '▁Product', '▁and', '▁ge', 'ography', '▁are', '▁what', '▁make', '▁cream', '▁ski', 'mming', '▁work', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "row = caching.ChunkedFilesDataCache(\"../../exp/cache/mnli/train\").load_chunk(0)[0][\"data_row\"]\n",
    "print(row.input_ids)\n",
    "print(row.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2n00e6Xrp1bI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0    165     72   1256  65017      4  22991    654   2394     48\n",
      "   9491      5      2      2   1004   1427   4240  10810  68901    142\n",
      "      4 216783     72   1312    745  27325   4223      6  84510      5\n",
      "      2      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1]\n",
      "['<s>', '▁und', '▁er', '▁hat', '▁gesagt', ',', '▁Mama', '▁ich', '▁bin', '▁da', 'heim', '.', '</s>', '</s>', '▁Er', '▁ri', 'ef', '▁seine', '▁Mutter', '▁an', ',', '▁sobald', '▁er', '▁aus', '▁dem', '▁Schul', 'bus', '▁', 'stieg', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "row = caching.ChunkedFilesDataCache(\"../../exp//cache/xnli_de/val\").load_chunk(0)[0][\"data_row\"]\n",
    "print(row.input_ids)\n",
    "print(row.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7FxZgEbqCx-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      6  49617      4  25710      4    631  43774    274     30\n",
      "      2      2      6   9889   3715 139030 137438   1826      4    852\n",
      "  40678  90629  25710 140576  28413     30      2      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1      1      1\n",
      "      1      1      1      1      1      1      1      1]\n",
      "['<s>', '▁', '他说', ',', '妈妈', ',', '我', '回来', '了', '。', '</s>', '</s>', '▁', '校', '车', '把他', '放下', '后', ',', '他', '立即', '给他', '妈妈', '打了', '电话', '。', '</s>']\n"
     ]
    }
   ],
   "source": [
    "row = caching.ChunkedFilesDataCache(\"../../exp/cache/xnli_zh/val\").load_chunk(0)[0][\"data_row\"]\n",
    "print(row.input_ids)\n",
    "print(row.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MBuH19IWOr0"
   },
   "source": [
    "#### Writing a run config\n",
    "\n",
    "Here we are going to write what we call a `jiant_task_container_config`. This configuration file basically defines a lot of the subtleties of our training pipeline, such as what tasks we will train on, do evaluation on, batch size for each task. The new version of `jiant` leans heavily toward explicitly specifying everything, for the purpose of inspectability and leaving minimal surprises for the user, even as the cost of being more verbose.\n",
    "\n",
    "We use a helper \"Configurator\" to write out a `jiant_task_container_config`, since most of our setup is pretty standard. We specify to train only on MNLI, but evaluate on MNLI, XNLI-de and XNLI-zh.\n",
    "\n",
    "**Depending on what GPU your Colab session is assigned to, you may need to lower the train batch size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQYtl7xTKsiP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"task_config_path_dict\": {\n",
      "    \"mnli\": \"../../exp/tasks/configs/mnli_config.json\",\n",
      "    \"xnli_de\": \"../../exp/tasks/configs/xnli_de_config.json\",\n",
      "    \"xnli_zh\": \"../../exp/tasks/configs/xnli_zh_config.json\"\n",
      "  },\n",
      "  \"task_cache_config_dict\": {\n",
      "    \"mnli\": {\n",
      "      \"train\": \"../../exp/cache/mnli/train\",\n",
      "      \"val\": \"../../exp/cache/mnli/val\",\n",
      "      \"val_labels\": \"../../exp/cache/mnli/val_labels\"\n",
      "    },\n",
      "    \"xnli_de\": {\n",
      "      \"val\": \"../../exp/cache/xnli_de/val\",\n",
      "      \"val_labels\": \"../../exp/cache/xnli_de/val_labels\"\n",
      "    },\n",
      "    \"xnli_zh\": {\n",
      "      \"val\": \"../../exp/cache/xnli_zh/val\",\n",
      "      \"val_labels\": \"../../exp/cache/xnli_zh/val_labels\"\n",
      "    }\n",
      "  },\n",
      "  \"sampler_config\": {\n",
      "    \"sampler_type\": \"ProportionalMultiTaskSampler\"\n",
      "  },\n",
      "  \"global_train_config\": {\n",
      "    \"max_steps\": 39270,\n",
      "    \"warmup_steps\": 3927\n",
      "  },\n",
      "  \"task_specific_configs_dict\": {\n",
      "    \"mnli\": {\n",
      "      \"train_batch_size\": 1,\n",
      "      \"eval_batch_size\": 1,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"eval_subset_num\": 500\n",
      "    },\n",
      "    \"xnli_de\": {\n",
      "      \"train_batch_size\": 1,\n",
      "      \"eval_batch_size\": 1,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"eval_subset_num\": 500\n",
      "    },\n",
      "    \"xnli_zh\": {\n",
      "      \"train_batch_size\": 1,\n",
      "      \"eval_batch_size\": 1,\n",
      "      \"gradient_accumulation_steps\": 1,\n",
      "      \"eval_subset_num\": 500\n",
      "    }\n",
      "  },\n",
      "  \"taskmodels_config\": {\n",
      "    \"task_to_taskmodel_map\": {\n",
      "      \"mnli\": \"mnli\",\n",
      "      \"xnli_de\": \"xnli_de\",\n",
      "      \"xnli_zh\": \"xnli_zh\"\n",
      "    },\n",
      "    \"taskmodel_config_map\": {\n",
      "      \"mnli\": null,\n",
      "      \"xnli_de\": null,\n",
      "      \"xnli_zh\": null\n",
      "    }\n",
      "  },\n",
      "  \"task_run_config\": {\n",
      "    \"train_task_list\": [\n",
      "      \"mnli\"\n",
      "    ],\n",
      "    \"train_val_task_list\": [\n",
      "      \"mnli\"\n",
      "    ],\n",
      "    \"val_task_list\": [\n",
      "      \"mnli\",\n",
      "      \"xnli_de\",\n",
      "      \"xnli_zh\"\n",
      "    ],\n",
      "    \"test_task_list\": []\n",
      "  },\n",
      "  \"metric_aggregator_config\": {\n",
      "    \"metric_aggregator_type\": \"EqualMetricAggregator\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "jiant_run_config = configurator.SimpleAPIMultiTaskConfigurator(\n",
    "    task_config_base_path=\"../../exp/tasks/configs\",\n",
    "    task_cache_base_path=\"../../exp/cache\",\n",
    "    train_task_name_list=[\"mnli\"],\n",
    "    val_task_name_list=[\"mnli\", \"xnli_de\", \"xnli_zh\"],\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=1,\n",
    "    epochs=0.1,\n",
    "    num_gpus=1,\n",
    ").create_config()\n",
    "display.show_json(jiant_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UF501yoXHBi"
   },
   "source": [
    "To briefly go over the major components of the `jiant_task_container_config`:\n",
    "\n",
    "* `task_config_path_dict`: The paths to the task config files we wrote above.\n",
    "* `task_cache_config_dict`: The paths to the task features caches we generated above.\n",
    "* `sampler_config`: Determines how to sample from different tasks during training.\n",
    "* `global_train_config`: The number of total steps and warmup steps during training.\n",
    "* `task_specific_configs_dict`: Task-specific arguments for each task, such as training batch size and gradient accumulation steps.\n",
    "* `taskmodels_config`: Task-model specific arguments for each task-model, including what tasks use which model.\n",
    "* `metric_aggregator_config`: Determines how to weight/aggregate the metrics across multiple tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf9C_RlBX-lK"
   },
   "source": [
    "**We need to make one small change to the auto-generated config**: we need to ensure that all three tasks use the same model head. Otherwise, each task will have its own task head, and the XNLI heads will be untrained.\n",
    "\n",
    "We can make a simple change to the dictionary, setting all of them to point to an `nli_model` head, and then write out the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InlkHBiCTXT9"
   },
   "outputs": [],
   "source": [
    "jiant_run_config[\"taskmodels_config\"][\"task_to_taskmodel_map\"] = {\n",
    "    \"mnli\": \"nli_model\",\n",
    "    \"xnli_de\": \"nli_model\",\n",
    "    \"xnli_zh\": \"nli_model\",\n",
    "}\n",
    "os.makedirs(\"./run_configs/\", exist_ok=True)\n",
    "py_io.write_json(jiant_run_config, \"./run_configs/jiant_run_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBKkvXzdYPqZ"
   },
   "source": [
    "#### Start training\n",
    "\n",
    "Finally, we can start our training run. \n",
    "\n",
    "Before starting training, the script also prints out the list of parameters in our model. You should notice that the only task head is the `nli_model` head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JdwWPgjQWx6I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  jiant_task_container_config_path: ./run_configs/jiant_run_config.json\n",
      "  output_dir: ./runs/run1\n",
      "  hf_pretrained_model_name_or_path: xlm-roberta-base\n",
      "  model_path: ./models/xlm-roberta-base/model/model.p\n",
      "  model_config_path: ./models/xlm-roberta-base/model/config.json\n",
      "  model_load_mode: from_transformers\n",
      "  do_train: True\n",
      "  do_val: True\n",
      "  do_save: False\n",
      "  do_save_last: False\n",
      "  do_save_best: False\n",
      "  write_val_preds: False\n",
      "  write_test_preds: False\n",
      "  eval_every_steps: 500\n",
      "  save_every_steps: 0\n",
      "  save_checkpoint_every_steps: 0\n",
      "  no_improvements_for_n_evals: 0\n",
      "  keep_checkpoint_when_done: False\n",
      "  force_overwrite: True\n",
      "  seed: -1\n",
      "  learning_rate: 1e-05\n",
      "  adam_epsilon: 1e-08\n",
      "  max_grad_norm: 1.0\n",
      "  optimizer_type: adam\n",
      "  freeze_layers: 0-8\n",
      "  no_cuda: False\n",
      "  fp16: False\n",
      "  fp16_opt_level: O1\n",
      "  local_rank: -1\n",
      "  server_ip: \n",
      "  server_port: \n",
      "device: cuda n_gpu: 3, distributed training: False, 16-bits training: False\n",
      "Using seed: 977026747\n",
      "{\n",
      "  \"jiant_task_container_config_path\": \"./run_configs/jiant_run_config.json\",\n",
      "  \"output_dir\": \"./runs/run1\",\n",
      "  \"hf_pretrained_model_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"model_path\": \"./models/xlm-roberta-base/model/model.p\",\n",
      "  \"model_config_path\": \"./models/xlm-roberta-base/model/config.json\",\n",
      "  \"model_load_mode\": \"from_transformers\",\n",
      "  \"do_train\": true,\n",
      "  \"do_val\": true,\n",
      "  \"do_save\": false,\n",
      "  \"do_save_last\": false,\n",
      "  \"do_save_best\": false,\n",
      "  \"write_val_preds\": false,\n",
      "  \"write_test_preds\": false,\n",
      "  \"eval_every_steps\": 500,\n",
      "  \"save_every_steps\": 0,\n",
      "  \"save_checkpoint_every_steps\": 0,\n",
      "  \"no_improvements_for_n_evals\": 0,\n",
      "  \"keep_checkpoint_when_done\": false,\n",
      "  \"force_overwrite\": true,\n",
      "  \"seed\": 977026747,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"adam_epsilon\": 1e-08,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"optimizer_type\": \"adam\",\n",
      "  \"freeze_layers\": \"0-8\",\n",
      "  \"no_cuda\": false,\n",
      "  \"fp16\": false,\n",
      "  \"fp16_opt_level\": \"O1\",\n",
      "  \"local_rank\": -1,\n",
      "  \"server_ip\": \"\",\n",
      "  \"server_port\": \"\"\n",
      "}\n",
      "3\n",
      "Creating Tasks:\n",
      "    mnli (MnliTask): ../../exp/tasks/configs/mnli_config.json\n",
      "    xnli_de (XnliTask): ../../exp/tasks/configs/xnli_de_config.json\n",
      "    xnli_zh (XnliTask): ../../exp/tasks/configs/xnli_zh_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/proj/main/modeling/model_setup.py:181: UserWarning: The following weights were not loaded: dict_keys(['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias'])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** freezing  encoder.layer.0.attention.self.query.weight\n",
      "******** freezing  encoder.layer.0.attention.self.query.bias\n",
      "******** freezing  encoder.layer.0.attention.self.key.weight\n",
      "******** freezing  encoder.layer.0.attention.self.key.bias\n",
      "******** freezing  encoder.layer.0.attention.self.value.weight\n",
      "******** freezing  encoder.layer.0.attention.self.value.bias\n",
      "******** freezing  encoder.layer.0.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.0.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.0.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.0.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.0.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.0.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.0.output.dense.weight\n",
      "******** freezing  encoder.layer.0.output.dense.bias\n",
      "******** freezing  encoder.layer.0.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.0.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.1.attention.self.query.weight\n",
      "******** freezing  encoder.layer.1.attention.self.query.bias\n",
      "******** freezing  encoder.layer.1.attention.self.key.weight\n",
      "******** freezing  encoder.layer.1.attention.self.key.bias\n",
      "******** freezing  encoder.layer.1.attention.self.value.weight\n",
      "******** freezing  encoder.layer.1.attention.self.value.bias\n",
      "******** freezing  encoder.layer.1.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.1.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.1.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.1.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.1.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.1.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.1.output.dense.weight\n",
      "******** freezing  encoder.layer.1.output.dense.bias\n",
      "******** freezing  encoder.layer.1.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.1.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.2.attention.self.query.weight\n",
      "******** freezing  encoder.layer.2.attention.self.query.bias\n",
      "******** freezing  encoder.layer.2.attention.self.key.weight\n",
      "******** freezing  encoder.layer.2.attention.self.key.bias\n",
      "******** freezing  encoder.layer.2.attention.self.value.weight\n",
      "******** freezing  encoder.layer.2.attention.self.value.bias\n",
      "******** freezing  encoder.layer.2.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.2.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.2.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.2.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.2.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.2.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.2.output.dense.weight\n",
      "******** freezing  encoder.layer.2.output.dense.bias\n",
      "******** freezing  encoder.layer.2.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.2.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.3.attention.self.query.weight\n",
      "******** freezing  encoder.layer.3.attention.self.query.bias\n",
      "******** freezing  encoder.layer.3.attention.self.key.weight\n",
      "******** freezing  encoder.layer.3.attention.self.key.bias\n",
      "******** freezing  encoder.layer.3.attention.self.value.weight\n",
      "******** freezing  encoder.layer.3.attention.self.value.bias\n",
      "******** freezing  encoder.layer.3.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.3.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.3.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.3.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.3.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.3.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.3.output.dense.weight\n",
      "******** freezing  encoder.layer.3.output.dense.bias\n",
      "******** freezing  encoder.layer.3.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.3.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.4.attention.self.query.weight\n",
      "******** freezing  encoder.layer.4.attention.self.query.bias\n",
      "******** freezing  encoder.layer.4.attention.self.key.weight\n",
      "******** freezing  encoder.layer.4.attention.self.key.bias\n",
      "******** freezing  encoder.layer.4.attention.self.value.weight\n",
      "******** freezing  encoder.layer.4.attention.self.value.bias\n",
      "******** freezing  encoder.layer.4.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.4.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.4.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.4.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.4.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.4.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.4.output.dense.weight\n",
      "******** freezing  encoder.layer.4.output.dense.bias\n",
      "******** freezing  encoder.layer.4.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.4.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.5.attention.self.query.weight\n",
      "******** freezing  encoder.layer.5.attention.self.query.bias\n",
      "******** freezing  encoder.layer.5.attention.self.key.weight\n",
      "******** freezing  encoder.layer.5.attention.self.key.bias\n",
      "******** freezing  encoder.layer.5.attention.self.value.weight\n",
      "******** freezing  encoder.layer.5.attention.self.value.bias\n",
      "******** freezing  encoder.layer.5.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.5.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.5.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.5.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.5.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.5.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.5.output.dense.weight\n",
      "******** freezing  encoder.layer.5.output.dense.bias\n",
      "******** freezing  encoder.layer.5.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.5.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.6.attention.self.query.weight\n",
      "******** freezing  encoder.layer.6.attention.self.query.bias\n",
      "******** freezing  encoder.layer.6.attention.self.key.weight\n",
      "******** freezing  encoder.layer.6.attention.self.key.bias\n",
      "******** freezing  encoder.layer.6.attention.self.value.weight\n",
      "******** freezing  encoder.layer.6.attention.self.value.bias\n",
      "******** freezing  encoder.layer.6.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.6.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.6.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.6.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.6.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.6.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.6.output.dense.weight\n",
      "******** freezing  encoder.layer.6.output.dense.bias\n",
      "******** freezing  encoder.layer.6.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.6.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.7.attention.self.query.weight\n",
      "******** freezing  encoder.layer.7.attention.self.query.bias\n",
      "******** freezing  encoder.layer.7.attention.self.key.weight\n",
      "******** freezing  encoder.layer.7.attention.self.key.bias\n",
      "******** freezing  encoder.layer.7.attention.self.value.weight\n",
      "******** freezing  encoder.layer.7.attention.self.value.bias\n",
      "******** freezing  encoder.layer.7.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.7.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.7.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.7.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.7.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.7.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.7.output.dense.weight\n",
      "******** freezing  encoder.layer.7.output.dense.bias\n",
      "******** freezing  encoder.layer.7.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.7.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.8.attention.self.query.weight\n",
      "******** freezing  encoder.layer.8.attention.self.query.bias\n",
      "******** freezing  encoder.layer.8.attention.self.key.weight\n",
      "******** freezing  encoder.layer.8.attention.self.key.bias\n",
      "******** freezing  encoder.layer.8.attention.self.value.weight\n",
      "******** freezing  encoder.layer.8.attention.self.value.bias\n",
      "******** freezing  encoder.layer.8.attention.output.dense.weight\n",
      "******** freezing  encoder.layer.8.attention.output.dense.bias\n",
      "******** freezing  encoder.layer.8.attention.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.8.attention.output.LayerNorm.bias\n",
      "******** freezing  encoder.layer.8.intermediate.dense.weight\n",
      "******** freezing  encoder.layer.8.intermediate.dense.bias\n",
      "******** freezing  encoder.layer.8.output.dense.weight\n",
      "******** freezing  encoder.layer.8.output.dense.bias\n",
      "******** freezing  encoder.layer.8.output.LayerNorm.weight\n",
      "******** freezing  encoder.layer.8.output.LayerNorm.bias\n",
      "No optimizer decay for:\n",
      "  encoder.embeddings.LayerNorm.weight\n",
      "  encoder.embeddings.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.attention.self.query.bias\n",
      "  encoder.encoder.layer.0.attention.self.key.bias\n",
      "  encoder.encoder.layer.0.attention.self.value.bias\n",
      "  encoder.encoder.layer.0.attention.output.dense.bias\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.0.intermediate.dense.bias\n",
      "  encoder.encoder.layer.0.output.dense.bias\n",
      "  encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.attention.self.query.bias\n",
      "  encoder.encoder.layer.1.attention.self.key.bias\n",
      "  encoder.encoder.layer.1.attention.self.value.bias\n",
      "  encoder.encoder.layer.1.attention.output.dense.bias\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.1.intermediate.dense.bias\n",
      "  encoder.encoder.layer.1.output.dense.bias\n",
      "  encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.attention.self.query.bias\n",
      "  encoder.encoder.layer.2.attention.self.key.bias\n",
      "  encoder.encoder.layer.2.attention.self.value.bias\n",
      "  encoder.encoder.layer.2.attention.output.dense.bias\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.2.intermediate.dense.bias\n",
      "  encoder.encoder.layer.2.output.dense.bias\n",
      "  encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.attention.self.query.bias\n",
      "  encoder.encoder.layer.3.attention.self.key.bias\n",
      "  encoder.encoder.layer.3.attention.self.value.bias\n",
      "  encoder.encoder.layer.3.attention.output.dense.bias\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.3.intermediate.dense.bias\n",
      "  encoder.encoder.layer.3.output.dense.bias\n",
      "  encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.attention.self.query.bias\n",
      "  encoder.encoder.layer.4.attention.self.key.bias\n",
      "  encoder.encoder.layer.4.attention.self.value.bias\n",
      "  encoder.encoder.layer.4.attention.output.dense.bias\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.4.intermediate.dense.bias\n",
      "  encoder.encoder.layer.4.output.dense.bias\n",
      "  encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.attention.self.query.bias\n",
      "  encoder.encoder.layer.5.attention.self.key.bias\n",
      "  encoder.encoder.layer.5.attention.self.value.bias\n",
      "  encoder.encoder.layer.5.attention.output.dense.bias\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.5.intermediate.dense.bias\n",
      "  encoder.encoder.layer.5.output.dense.bias\n",
      "  encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.attention.self.query.bias\n",
      "  encoder.encoder.layer.6.attention.self.key.bias\n",
      "  encoder.encoder.layer.6.attention.self.value.bias\n",
      "  encoder.encoder.layer.6.attention.output.dense.bias\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.6.intermediate.dense.bias\n",
      "  encoder.encoder.layer.6.output.dense.bias\n",
      "  encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.attention.self.query.bias\n",
      "  encoder.encoder.layer.7.attention.self.key.bias\n",
      "  encoder.encoder.layer.7.attention.self.value.bias\n",
      "  encoder.encoder.layer.7.attention.output.dense.bias\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.7.intermediate.dense.bias\n",
      "  encoder.encoder.layer.7.output.dense.bias\n",
      "  encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.attention.self.query.bias\n",
      "  encoder.encoder.layer.8.attention.self.key.bias\n",
      "  encoder.encoder.layer.8.attention.self.value.bias\n",
      "  encoder.encoder.layer.8.attention.output.dense.bias\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.8.intermediate.dense.bias\n",
      "  encoder.encoder.layer.8.output.dense.bias\n",
      "  encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.attention.self.query.bias\n",
      "  encoder.encoder.layer.9.attention.self.key.bias\n",
      "  encoder.encoder.layer.9.attention.self.value.bias\n",
      "  encoder.encoder.layer.9.attention.output.dense.bias\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.9.intermediate.dense.bias\n",
      "  encoder.encoder.layer.9.output.dense.bias\n",
      "  encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.attention.self.query.bias\n",
      "  encoder.encoder.layer.10.attention.self.key.bias\n",
      "  encoder.encoder.layer.10.attention.self.value.bias\n",
      "  encoder.encoder.layer.10.attention.output.dense.bias\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.10.intermediate.dense.bias\n",
      "  encoder.encoder.layer.10.output.dense.bias\n",
      "  encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.attention.self.query.bias\n",
      "  encoder.encoder.layer.11.attention.self.key.bias\n",
      "  encoder.encoder.layer.11.attention.self.value.bias\n",
      "  encoder.encoder.layer.11.attention.output.dense.bias\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "  encoder.encoder.layer.11.intermediate.dense.bias\n",
      "  encoder.encoder.layer.11.output.dense.bias\n",
      "  encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "  encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "  encoder.pooler.dense.bias\n",
      "  taskmodels_dict.nli_model.head.dense.bias\n",
      "  taskmodels_dict.nli_model.head.out_proj.bias\n",
      "Using AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/muller/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6a3328c744446e96b62b1a5c8b1c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/39270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 2.20 GiB already allocated; 20.56 MiB free; 2.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb Cell 27\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaison/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run_args \u001b[39m=\u001b[39m main_runscript\u001b[39m.\u001b[39mRunConfiguration(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaison/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     jiant_task_container_config_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./run_configs/jiant_run_config.json\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaison/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./runs/run1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaison/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     force_overwrite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaison/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsaison/home/muller/Devel/jiant/examples/notebooks/jiant_XNLI_Example.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m main_runscript\u001b[39m.\u001b[39mrun_loop(run_args)\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/proj/main/runscript.py:199\u001b[0m, in \u001b[0;36mrun_loop\u001b[0;34m(args, checkpoint)\u001b[0m\n\u001b[1;32m    197\u001b[0m         metarunner\u001b[39m.\u001b[39mload_state(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mmetarunner_state\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    198\u001b[0m         \u001b[39mdel\u001b[39;00m checkpoint[\u001b[39m\"\u001b[39m\u001b[39mmetarunner_state\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 199\u001b[0m     metarunner\u001b[39m.\u001b[39;49mrun_train_loop()\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdo_val:\n\u001b[1;32m    202\u001b[0m     val_results_dict \u001b[39m=\u001b[39m runner\u001b[39m.\u001b[39mrun_val(\n\u001b[1;32m    203\u001b[0m         task_name_list\u001b[39m=\u001b[39mrunner\u001b[39m.\u001b[39mjiant_task_container\u001b[39m.\u001b[39mtask_run_config\u001b[39m.\u001b[39mval_task_list,\n\u001b[1;32m    204\u001b[0m         return_preds\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mwrite_val_preds,\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/shared/metarunner.py:38\u001b[0m, in \u001b[0;36mAbstractMetarunner.run_train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_train_loop\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbegin_training()\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myield_train_step():\n\u001b[1;32m     39\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_save_model():\n\u001b[1;32m     40\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model()\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/proj/main/metarunner.py:112\u001b[0m, in \u001b[0;36mJiantMetarunner.yield_train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     train_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mresume_train_context(\n\u001b[1;32m    109\u001b[0m         train_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state,\n\u001b[1;32m    110\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m     )\n\u001b[0;32m--> 112\u001b[0m \u001b[39mfor\u001b[39;00m train_state \u001b[39min\u001b[39;00m train_iterator:\n\u001b[1;32m    113\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_state \u001b[39m=\u001b[39m train_state\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minject_at_step()\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/proj/main/runner.py:75\u001b[0m, in \u001b[0;36mJiantRunner.run_train_context\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m train_state \u001b[39m=\u001b[39m TrainState\u001b[39m.\u001b[39mfrom_task_name_list(\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjiant_task_container\u001b[39m.\u001b[39mtask_run_config\u001b[39m.\u001b[39mtrain_task_list\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m maybe_tqdm(\n\u001b[1;32m     71\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjiant_task_container\u001b[39m.\u001b[39mglobal_train_config\u001b[39m.\u001b[39mmax_steps),\n\u001b[1;32m     72\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m     74\u001b[0m ):\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_train_step(\n\u001b[1;32m     76\u001b[0m         train_dataloader_dict\u001b[39m=\u001b[39;49mtrain_dataloader_dict, train_state\u001b[39m=\u001b[39;49mtrain_state\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m     \u001b[39myield\u001b[39;00m train_state\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/proj/main/runner.py:110\u001b[0m, in \u001b[0;36mJiantRunner.run_train_step\u001b[0;34m(self, train_dataloader_dict, train_state)\u001b[0m\n\u001b[1;32m    103\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    104\u001b[0m     model_output \u001b[39m=\u001b[39m wrap_jiant_forward(\n\u001b[1;32m    105\u001b[0m         jiant_model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjiant_model,\n\u001b[1;32m    106\u001b[0m         batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m    107\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[1;32m    108\u001b[0m         compute_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m     )\n\u001b[0;32m--> 110\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcomplex_backpropagate(\n\u001b[1;32m    111\u001b[0m         loss\u001b[39m=\u001b[39;49mmodel_output\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    112\u001b[0m         gradient_accumulation_steps\u001b[39m=\u001b[39;49mtask_specific_config\u001b[39m.\u001b[39;49mgradient_accumulation_steps,\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     loss_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/proj/main/runner.py:224\u001b[0m, in \u001b[0;36mJiantRunner.complex_backpropagate\u001b[0;34m(self, loss, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplex_backpropagate\u001b[39m(\u001b[39mself\u001b[39m, loss, gradient_accumulation_steps):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m complex_backpropagate(\n\u001b[1;32m    225\u001b[0m         loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m    226\u001b[0m         optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_scheduler\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    227\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjiant_model,\n\u001b[1;32m    228\u001b[0m         fp16\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrparams\u001b[39m.\u001b[39;49mfp16,\n\u001b[1;32m    229\u001b[0m         n_gpu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrparams\u001b[39m.\u001b[39;49mn_gpu,\n\u001b[1;32m    230\u001b[0m         gradient_accumulation_steps\u001b[39m=\u001b[39;49mgradient_accumulation_steps,\n\u001b[1;32m    231\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrparams\u001b[39m.\u001b[39;49mmax_grad_norm,\n\u001b[1;32m    232\u001b[0m     )\n",
      "File \u001b[0;32m/moredata/muller/Devel/jiant/examples/notebooks/../../../jiant/jiant/shared/runner.py:27\u001b[0m, in \u001b[0;36mcomplex_backpropagate\u001b[0;34m(loss, optimizer, model, fp16, n_gpu, gradient_accumulation_steps, max_grad_norm)\u001b[0m\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(amp\u001b[39m.\u001b[39mmaster_params(optimizer), max_grad_norm)\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_grad_norm)\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:34\u001b[0m, in \u001b[0;36mBroadcast.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(ctx, \u001b[39m*\u001b[39mgrad_outputs):\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m,) \u001b[39m+\u001b[39m ReduceAddCoalesced\u001b[39m.\u001b[39;49mapply(ctx\u001b[39m.\u001b[39;49minput_device, ctx\u001b[39m.\u001b[39;49mnum_inputs, \u001b[39m*\u001b[39;49mgrad_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:45\u001b[0m, in \u001b[0;36mReduceAddCoalesced.forward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     41\u001b[0m ctx\u001b[39m.\u001b[39mtarget_gpus \u001b[39m=\u001b[39m [grads[i]\u001b[39m.\u001b[39mget_device() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(grads), num_inputs)]\n\u001b[1;32m     43\u001b[0m grads_ \u001b[39m=\u001b[39m [grads[i:i \u001b[39m+\u001b[39m num_inputs]\n\u001b[1;32m     44\u001b[0m           \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(grads), num_inputs)]\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m comm\u001b[39m.\u001b[39;49mreduce_add_coalesced(grads_, destination)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/nn/parallel/comm.py:142\u001b[0m, in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m# now the dense ones, which have consistent sizes\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m chunks \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mitrs):\n\u001b[0;32m--> 142\u001b[0m     flat_tensors \u001b[39m=\u001b[39m [_flatten_dense_tensors(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]  \u001b[39m# (num_gpus,)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     flat_result \u001b[39m=\u001b[39m reduce_add(flat_tensors, destination)\n\u001b[1;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m _unflatten_dense_tensors(flat_result, chunks[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    145\u001b[0m         \u001b[39m# The unflattened tensors do not share storage, and we don't expose\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[39m# base flat tensor anyways, so give them different version counters.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[39m# See NOTE [ Version Counter in comm.*_coalesced ]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/nn/parallel/comm.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m# now the dense ones, which have consistent sizes\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m chunks \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mitrs):\n\u001b[0;32m--> 142\u001b[0m     flat_tensors \u001b[39m=\u001b[39m [_flatten_dense_tensors(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]  \u001b[39m# (num_gpus,)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     flat_result \u001b[39m=\u001b[39m reduce_add(flat_tensors, destination)\n\u001b[1;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m _unflatten_dense_tensors(flat_result, chunks[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    145\u001b[0m         \u001b[39m# The unflattened tensors do not share storage, and we don't expose\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[39m# base flat tensor anyways, so give them different version counters.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[39m# See NOTE [ Version Counter in comm.*_coalesced ]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jiant3.8/lib/python3.8/site-packages/torch/_utils.py:292\u001b[0m, in \u001b[0;36m_flatten_dense_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_dense_tensors\u001b[39m(tensors):\n\u001b[1;32m    279\u001b[0m     \u001b[39m\"\"\"Flatten dense tensors into a contiguous 1D buffer. Assume tensors are of\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39m    same dense type.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m        A contiguous 1D buffer containing input tensors.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mflatten_dense_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 2.20 GiB already allocated; 20.56 MiB free; 2.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "run_args = main_runscript.RunConfiguration(\n",
    "    jiant_task_container_config_path=\"./run_configs/jiant_run_config.json\",\n",
    "    output_dir=\"./runs/run1\",\n",
    "    hf_pretrained_model_name_or_path=\"xlm-roberta-base\",\n",
    "    model_path=\"./models/xlm-roberta-base/model/model.p\",\n",
    "    model_config_path=\"./models/xlm-roberta-base/model/config.json\",\n",
    "    learning_rate=1e-5,\n",
    "    eval_every_steps=500,\n",
    "    do_train=True,\n",
    "    do_val=True,\n",
    "    force_overwrite=True,\n",
    ")\n",
    "\n",
    "main_runscript.run_loop(run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SXcuHFIYp6Y"
   },
   "source": [
    "Finally, we should see the validation scores for MNLI, XNLI-de, and XNLI-zh. Given that the training data is in English, we expect to see slightly higher scores for MNLI, but the scores for XNLI-de and XNLI-zh are still decent!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "jiant XNLI Example",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
